{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Process with Spark - ETL"
      ],
      "metadata": {
        "id": "kp9tbSVbj18h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, max, min, when, lag, mean, unix_timestamp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL script for TalkingData AdTracking Fraud Detection using PySpark\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "NiLtlALVj3OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read raw data\n",
        "raw_data = spark.read.csv('train_subset.csv', header=True, inferSchema=True)\n",
        "raw_data.show(3)"
      ],
      "metadata": {
        "id": "uIb4T1wFZNsA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "window_ip = Window.partitionBy(\"ip\").orderBy('click_time')\n",
        "\n",
        "data = raw_data.withColumn(\"IP_first_click\", when(min(\"click_time\").over(window_ip) == col(\"click_time\"), 1).otherwise(0)) \\\n",
        "                .withColumn(\"IP_click_count\", count(\"*\").over(window_ip)) \\\n",
        "                .withColumn(\"IP_download_count\", sum(\"is_attributed\").over(window_ip)) \\\n",
        "                .withColumn(\"IP_download_ratio\", col(\"IP_download_count\") / col(\"IP_click_count\")) \\\n",
        "                .withColumn(\"last_click_time\", lag(\"click_time\", 1).over(window_ip)) \\\n",
        "                .withColumn(\"IP_time_since_last_click\", when(col(\"last_click_time\").isNull(), -1).otherwise((unix_timestamp(col(\"click_time\")) - unix_timestamp(col(\"last_click_time\")))))\n",
        "data.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZtyo-wdZq55",
        "outputId": "cdccc50f-3170-4e19-b556-ddb92762b8b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+------+---+-------+-------------------+-------------+--------------+--------------+-----------------+-----------------+-------------------+------------------------+\n",
            "| ip|app|device| os|channel|         click_time|is_attributed|IP_first_click|IP_click_count|IP_download_count|IP_download_ratio|    last_click_time|IP_time_since_last_click|\n",
            "+---+---+------+---+-------+-------------------+-------------+--------------+--------------+-----------------+-----------------+-------------------+------------------------+\n",
            "|  9| 18|     1| 13|    107|2017-11-06 16:02:30|            0|             1|             1|                0|              0.0|               null|                      -1|\n",
            "|  9| 18|     1| 13|    107|2017-11-06 16:02:41|            0|             0|             2|                0|              0.0|2017-11-06 16:02:30|                      11|\n",
            "|  9| 18|     1| 13|    107|2017-11-06 16:24:54|            0|             0|             3|                0|              0.0|2017-11-06 16:02:41|                    1333|\n",
            "+---+---+------+---+-------+-------------------+-------------+--------------+--------------+-----------------+-----------------+-------------------+------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate IP address statistics - will be used for prediction\n",
        "ip_stats = data.groupBy('ip').agg(\n",
        "    count('*').alias('IP_click_count'),\n",
        "    sum('is_attributed').alias('IP_download_count'),\n",
        "    (sum('is_attributed') / count('*')).alias('IP_download_ratio'),\n",
        "    max('click_time').alias('last_click_time'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHY0XSIHZVuA",
        "outputId": "f55802c4-5b40-4f6b-f801-6a8db3022f64"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+-----------------+--------------------+-------------------+\n",
            "|    ip|IP_click_count|IP_download_count|   IP_download_ratio|    last_click_time|\n",
            "+------+--------------+-----------------+--------------------+-------------------+\n",
            "| 50223|          1137|                2|0.001759014951627089|2017-11-06 18:31:11|\n",
            "|149177|            24|                0|                 0.0|2017-11-06 16:53:42|\n",
            "| 37489|            87|                0|                 0.0|2017-11-06 18:27:43|\n",
            "+------+--------------+-----------------+--------------------+-------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine the code into ETL script and test output"
      ],
      "metadata": {
        "id": "YoJVWcVHjop4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile etl_spark.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, max, min, when, lag, mean, unix_timestamp\n",
        "from pyspark.sql.window import Window\n",
        "import argparse\n",
        "\n",
        "def main(args):\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"ETL script for TalkingData AdTracking Fraud Detection using PySpark\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Read raw data\n",
        "    raw_data = spark.read.csv(args.input_file, header=True, inferSchema=True)\n",
        "\n",
        "    # Feature engineering\n",
        "    window_ip = Window.partitionBy(\"ip\").orderBy('click_time')\n",
        "\n",
        "    data = raw_data.withColumn(\"IP_first_click\", when(min(\"click_time\").over(window_ip) == col(\"click_time\"), 1).otherwise(0)) \\\n",
        "                .withColumn(\"IP_click_count\", count(\"*\").over(window_ip)) \\\n",
        "                .withColumn(\"IP_download_count\", sum(\"is_attributed\").over(window_ip)) \\\n",
        "                .withColumn(\"IP_download_ratio\", col(\"IP_download_count\") / col(\"IP_click_count\")) \\\n",
        "                .withColumn(\"last_click_time\", lag(\"click_time\", 1).over(window_ip)) \\\n",
        "                .withColumn(\"IP_time_since_last_click\", when(col(\"last_click_time\").isNull(), -1).otherwise((unix_timestamp(col(\"click_time\")) - unix_timestamp(col(\"last_click_time\")))))\n",
        "\n",
        "    # Calculate IP address statistics\n",
        "    ip_stats = data.groupBy('ip').agg(\n",
        "        count('*').alias('IP_click_count'),\n",
        "        sum('is_attributed').alias('IP_download_count'),\n",
        "        (sum('is_attributed') / count('*')).alias('IP_download_ratio'),\n",
        "        max('click_time').alias('last_click_time')\n",
        "    )\n",
        "\n",
        "    # Save the processed data\n",
        "    data.write.parquet(args.processed_data_output)\n",
        "\n",
        "    # Save the IP address statistics\n",
        "    ip_stats.write.parquet(args.ip_stats_output)\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='ETL script for TalkingData AdTracking Fraud Detection using PySpark')\n",
        "    parser.add_argument('--input_file', type=str, required=True, help='Input file path for raw data')\n",
        "    parser.add_argument('--processed_data_output', type=str, required=True, help='Output file path for processed data')\n",
        "    parser.add_argument('--ip_stats_output', type=str, required=True, help='Output file path for IP address statistics')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLoNcFDRXaZE",
        "outputId": "7548312d-c126-4ada-dfa6-15687d3efe79"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting etl_spark.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%python3 /content/etl_spark.py --input_file 'train_subset.csv' --processed_data_output 'train_output' --ip_stats_output 'ip_stats_output'"
      ],
      "metadata": {
        "id": "HkVcpgUr40Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the parquet files from the processed_data_output folder into a DataFrame\n",
        "processed_data = spark.read.parquet(\"train_output\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "processed_data.show(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3B1TBhqYrmP",
        "outputId": "37207bd5-766b-4e5e-8b8b-529c37eb5b3a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+------+---+-------+-------------------+-------------+--------------+--------------+-----------------+-----------------+-------------------+------------------------+\n",
            "| ip|app|device| os|channel|         click_time|is_attributed|IP_first_click|IP_click_count|IP_download_count|IP_download_ratio|    last_click_time|IP_time_since_last_click|\n",
            "+---+---+------+---+-------+-------------------+-------------+--------------+--------------+-----------------+-----------------+-------------------+------------------------+\n",
            "| 10| 64|     1| 22|    459|2017-11-06 16:06:34|            0|             1|             1|                0|              0.0|               null|                      -1|\n",
            "| 10| 18|     1| 19|    121|2017-11-06 16:22:46|            0|             0|             3|                0|              0.0|2017-11-06 16:06:34|                     972|\n",
            "| 10| 15|     1| 19|    245|2017-11-06 16:22:46|            0|             0|             3|                0|              0.0|2017-11-06 16:22:46|                       0|\n",
            "+---+---+------+---+-------+-------------------+-------------+--------------+--------------+-----------------+-----------------+-------------------+------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train with Spark"
      ],
      "metadata": {
        "id": "h1g3LG4kjvxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Feature columns\n",
        "feature_columns = ['app', 'device', 'os', 'channel', \"IP_first_click\", \"IP_time_since_last_click\", \"IP_click_count\", \"IP_download_ratio\"]\n",
        "\n",
        "# Prepare features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "processed_data = assembler.transform(processed_data)\n",
        "\n",
        "# Train-test split\n",
        "train_data, test_data = processed_data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Train the model\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_attributed\", numTrees=100, seed=42)\n",
        "model = rf.fit(train_data)\n",
        "\n",
        "# Model evaluation\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"is_attributed\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mhdP0SJjGpd",
        "outputId": "cb71f2f3-48b1-43c3-a0ba-5dc17608e98a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9876090623850763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the script for training and evaluation"
      ],
      "metadata": {
        "id": "4nIsYUnC4ZJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_rf.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import argparse\n",
        "\n",
        "def main(args):\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Training script for TalkingData AdTracking Fraud Detection using PySpark\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Read processed data\n",
        "    processed_data = spark.read.parquet(args.processed_data_input)\n",
        "\n",
        "    # Feature columns\n",
        "    feature_columns = ['app', 'device', 'os', 'channel', \"IP_first_click\", \"IP_time_since_last_click\", \"IP_click_count\", \"IP_download_ratio\"]\n",
        "\n",
        "    # Prepare features\n",
        "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "    processed_data = assembler.transform(processed_data)\n",
        "\n",
        "    # Train-test split\n",
        "    train_data, test_data = processed_data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "    # Train the model\n",
        "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_attributed\", numTrees=100, seed=42)\n",
        "    model = rf.fit(train_data)\n",
        "\n",
        "    # Model evaluation\n",
        "    predictions = model.transform(test_data)\n",
        "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"is_attributed\", metricName=\"areaUnderROC\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    print(f\"AUC: {auc}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.write().overwrite().save(args.model_output_path)\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Training script for TalkingData AdTracking Fraud Detection using PySpark')\n",
        "    parser.add_argument('--processed_data_input', type=str, required=True, help='Input file path for processed data')\n",
        "    parser.add_argument('--model_output_path', type=str, required=True, help='Output path for the trained model')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_ozW6bZmG-6",
        "outputId": "f878f5d4-28d7-445c-c4da-d28fff0d5b53"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_rf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%python3 train_rf.py --processed_data_input 'train_output' --model_output_path 'model_output'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG7jCISVqPMG",
        "outputId": "e261579d-778a-42f6-de45-518a90225ce2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9875399297370214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/04/24 05:12:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/04/24 05:12:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 1:>                                                          (0 + 2) / 2]\r\r[Stage 1:=============================>                             (1 + 1) / 2]\r\r                                                                                \r\r[Stage 4:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 5:>                                                          (0 + 2) / 2]\r\r                                                                                \r\r[Stage 6:>                                                          (0 + 2) / 2]\r\r[Stage 6:=============================>                             (1 + 1) / 2]\r\r                                                                                \r\r[Stage 8:>                                                          (0 + 2) / 2]\r23/04/24 05:14:24 WARN MemoryStore: Not enough space to cache rdd_30_0 in memory! (computed 41.9 MiB so far)\n",
            "23/04/24 05:14:25 WARN BlockManager: Persisting block rdd_30_0 to disk instead.\n",
            "23/04/24 05:14:26 WARN MemoryStore: Not enough space to cache rdd_30_1 in memory! (computed 62.9 MiB so far)\n",
            "23/04/24 05:14:26 WARN BlockManager: Persisting block rdd_30_1 to disk instead.\n",
            "\r[Stage 8:>                                                          (0 + 2) / 2]\r23/04/24 05:15:13 WARN MemoryStore: Not enough space to cache rdd_30_0 in memory! (computed 147.2 MiB so far)\n",
            "23/04/24 05:15:14 WARN MemoryStore: Not enough space to cache rdd_30_1 in memory! (computed 226.4 MiB so far)\n",
            "\r[Stage 8:=============================>                             (1 + 1) / 2]\r\r                                                                                \r\r[Stage 10:>                                                         (0 + 2) / 2]\r23/04/24 05:15:48 WARN MemoryStore: Not enough space to cache rdd_30_1 in memory! (computed 147.2 MiB so far)\n",
            "23/04/24 05:15:49 WARN MemoryStore: Not enough space to cache rdd_30_0 in memory! (computed 226.4 MiB so far)\n",
            "\r[Stage 10:=============================>                            (1 + 1) / 2]\r\r                                                                                \r\r[Stage 12:>                                                         (0 + 2) / 2]\r23/04/24 05:16:28 WARN MemoryStore: Not enough space to cache rdd_30_0 in memory! (computed 147.2 MiB so far)\n",
            "23/04/24 05:16:29 WARN MemoryStore: Not enough space to cache rdd_30_1 in memory! (computed 226.4 MiB so far)\n",
            "\r[Stage 12:>                                                         (0 + 2) / 2]\r\r[Stage 12:=============================>                            (1 + 1) / 2]\r\r                                                                                \r\r[Stage 14:>                                                         (0 + 2) / 2]\r23/04/24 05:17:35 WARN MemoryStore: Not enough space to cache rdd_30_0 in memory! (computed 147.2 MiB so far)\n",
            "23/04/24 05:17:36 WARN MemoryStore: Not enough space to cache rdd_30_1 in memory! (computed 226.4 MiB so far)\n",
            "\r[Stage 14:=============================>                            (1 + 1) / 2]\r\r                                                                                \r\r[Stage 16:>                                                         (0 + 2) / 2]\r23/04/24 05:18:26 WARN MemoryStore: Not enough space to cache rdd_30_0 in memory! (computed 147.2 MiB so far)\n",
            "23/04/24 05:18:27 WARN MemoryStore: Not enough space to cache rdd_30_1 in memory! (computed 226.4 MiB so far)\n",
            "\r[Stage 16:=============================>                            (1 + 1) / 2]\r\r                                                                                \r\r[Stage 18:>                                                         (0 + 2) / 2]\r\r[Stage 18:=============================>                            (1 + 1) / 2]\r\r                                                                                \r"
          ]
        }
      ]
    }
  ]
}